<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      YOLO | Algotechniq 
    
  
</title>
<meta name="author" content="Algotechniq ">
<meta name="description" content="Illustrating a fast and accurate object detection algorithm">

  <meta name="keywords" content="on-road anomaly detection, structural damage detection, motorist safety, computer vision, machine learning, deep learning, transfer learning, artificial intelligence, Nvidia, Orin, Automatic Claim Estimator, Insurance, Vehicle Damage Assessment, Computer Vision, Deep Learning, Instance Segmentation, Mask R-CNN, Object Detection, Vehicle Damaged Detection, pavement distresses, road condition monitoring, deep learning in road damage detection, built-in vehicle cameras, GPS sensors in road condition monitoring, pavement damage detection using deep learning, machine learning in road damage detection, Algorithms, Design, Experimentation, Measurement, computer vision (CV), edge computing, Internet of things (IoT), graphical processing unit (GPU), single-board computers (SBCs), smart cities">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="/assets/img/faviconalgo.webp?e023f3f8fee08ff888d2356ef91d4268">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://www.algotechniq.com/blog/2024/yolov8/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Algotechniq</span>
            
            
            
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">home
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/projects/">projects
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/cv/">about
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">







<div class="post">
  <header class="post-header">
    <h1 class="post-title">YOLO</h1>
    <p class="post-meta">
      Created in August 12, 2024
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>
      
      
          ·  
        
          
            <a href="/blog/tag/technote"> <i class="fa-solid fa-hashtag fa-sm"></i> technote</a>
          
          
        
      

      
          ·  
        
          
            <a href="/blog/category/algoblog">
              <i class="fa-solid fa-tag fa-sm"></i> Algoblog</a>
          
          
        
      
    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>The “You Only Look Once” (YOLO) series of object detection models has been instrumental in advancing the field of computer vision, particularly in real-time object detection. The timeline of YOLO’s development highlights its evolution from a fast yet less precise model into a series of highly efficient and accurate tools.</p>

<h3 id="introduction-of-yolo-2016">Introduction of YOLO (2016)</h3>

<p>The original YOLO model, introduced in 2016 by Joseph Redmon and collaborators, shook up the machine learning field by treating object detection as a single regression problem <a class="citation" href="#Redmon2016CVPR">(Redmon et al., 2016)</a>. This innovative approach allowed YOLO to process images in one pass, resulting in excellent detection speeds. However, the initial model had limitations in accuracy, particularly with small or densely packed objects.</p>

<h3 id="interim-evolutions-2017-to-2020">Interim Evolutions: 2017 to 2020</h3>

<p>YOLOv2, released in 2017, improved on the original by adding anchor boxes, batch normalization, and a refined loss function. These changes boosted accuracy while keeping the model fast. YOLO9000, an extension of YOLOv2, could detect over 9,000 object categories, showing YOLO’s ability to handle large datasets.</p>

<p>YOLOv3 enhanced the model further with multi-scale detection, better handling objects of different sizes. The new Darknet-53 architecture improved feature extraction, and logistic regression enhanced accuracy, particularly for small and overlapping objects. YOLOv3 maintained a good balance between speed and accuracy, making it widely used.</p>

<p>YOLOv4, developed by Alexey Bochkovskiy <a class="citation" href="#bochkovskiy2020yolov4">(Bochkovskiy et al., 2020)</a>, introduced techniques like Cross Stage Partial (CSP) connections and self-adversarial training (SAT). These advancements increased both accuracy and robustness, while still prioritizing speed, reinforcing YOLO’s strong reputation.</p>

<h3 id="the-newest-models-v5-and-v8">The Newest Models: V5 and V8</h3>

<p>YOLOv5, released in 2020 by the <a href="https://github.com/ultralytics/ultralytics" rel="external nofollow noopener" target="_blank">Ultralytics</a> team, marked a significant shift in the YOLO series. Although not developed by the original authors, YOLOv5 quickly gained widespread adoption due to its ease of use, integration with PyTorch, and extensive documentation. It emphasized a more practical approach, optimizing the model for deployment in various real-world scenarios. YOLOv5 became a favorite among developers for its balance of simplicity, performance, and versatility, making it a go-to choice for many object detection tasks.</p>

<p>YOLOv8, released in 2023, represents the latest advancement in the YOLO series. It introduces cutting-edge features and enhancements in network architecture, further refining the model’s speed and accuracy. YOLOv8 is designed for seamless integration with modern hardware and software environments, making it highly adaptable and easy to deploy across a wide range of applications. This version solidifies YOLO’s position at the forefront of object detection technology, continuing the series’ tradition of innovation and excellence.</p>

<blockquote>
  <p>YOLO Series: Improving Speed and Precision</p>
</blockquote>

<p>The YOLO series has consistently pushed the boundaries of object detection, evolving from an efficient but limited model into a series of highly sophisticated tools. YOLOv5 and YOLOv8, in particular, have become essential in real-world applications, offering unparalleled performance and ease of use. As the series continues to develop, YOLO remains a leading choice for both researchers and practitioners in the field of computer vision.</p>

<h3 id="yolov8-in-depth">YOLOv8 In-Depth</h3>

<p>To get insight into the function of YOLOv8, let’s look at these two picture:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
    <img src="/assets/img/puppies0.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
    <img src="/assets/img/puppies1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
The input (left) and the output of an example to illustrate the function of YOLOv8. 
</div>

<p>The one on the left would be the input to YOLOv8 and the right with the highlighted bounding boxes would be the output.</p>

<blockquote>
  <p>The function of YOLOv8 for object detection is to determine the bounding boxes for different objects in an image.</p>
</blockquote>

<p>Let’s describe the three principal components of the YOLOv8 architecture using some algebra. There are the Backbone, the Neck, and Head.</p>

<h4 id="backbone">Backbone</h4>

<p>The backbone is a convolutional neural network that extracts feature maps from the input image. Let’s denote the input image as \( X \) with dimensions \( H \times W \times C \) (height \( H \), width \( W \), and \( C \) channels).</p>

<p>The backbone can be represented as a series of convolutional operations followed by non-linear activation functions and possibly downsampling operations (e.g., pooling or strided convolutions). Mathematically, if the backbone consists of \( L \) layers, it can be represented as:</p>

\[F_l = \sigma(W_l \ast F_{l-1} + b_l), \quad l = 1, 2, \dots, L\]

<p>Where:</p>
<ul>
  <li>\( F_0 = X \) is the input image.</li>
  <li>\( W_l \) and \( b_l \) are the weights and biases of the \( l \)-th layer.</li>
  <li>\( \ast \) denotes the convolution operation.</li>
  <li>\( \sigma(\cdot) \) is a non-linear activation function (e.g., ReLU).</li>
  <li>\( F_l \) is the feature map output by the \( l \)-th layer.</li>
</ul>

<p>The final output of the backbone, \( F_L \), is a set of feature maps \( {F_L^1, F_L^2, \dots, F_L^n} \) where each \( F_L^i \) corresponds to a feature map at different scales (e.g., multi-resolution feature maps for small, medium, and large objects).</p>

<h4 id="neck">Neck</h4>

<p>The neck is responsible for aggregating features at different scales. This can be represented using a Feature Pyramid Network (FPN) or a Path Aggregation Network (PANet), which combine feature maps from different layers to produce refined feature maps.</p>

<p>For simplicity, let’s assume the neck consists of two operations: upsampling and lateral connections from the backbone. The neck can be represented as:</p>

\[P_k = \text{Upsample}(F_{L}^k) + \text{Conv}(F_{L-1}^k), \quad k = 1, 2, \dots, n\]

<p>Where:</p>
<ul>
  <li>\( \text{Upsample}(\cdot) \) is an upsampling operation to match the resolution of the lower-level features.</li>
  <li>\( \text{Conv}(\cdot) \) represents a convolutional operation that adjusts the number of channels or refines features.</li>
  <li>\( P_k \) is the feature map after aggregation at scale \( k \).</li>
</ul>

<p>The final output of the neck is a set of refined feature maps \( {P_1, P_2, \dots, P_n} \), which combine information across multiple scales.</p>

<h4 id="head">Head</h4>

<p>The head is responsible for generating the final predictions, including bounding boxes, objectness scores, and class probabilities. The head operates on the feature maps output by the neck.</p>

<p>For each scale \( k \), the head can be represented as:</p>

\[B_k = \text{Conv}_\text{bbox}(P_k), \quad C_k = \text{Conv}_\text{class}(P_k)\]

<p>Where:</p>
<ul>
  <li>
    <p>\( B_k \) represents the bounding box predictions for scale \( k \), including coordinates \( (x, y, w, h) \).</p>
  </li>
  <li>
    <p>\( C_k \) represents the class probabilities and objectness score predictions for scale \( k \).</p>
  </li>
  <li>
    <p>\(\text{Conv}_\text{bbox}(\cdot) \\) and \\( \text{Conv}_\text{class}(\cdot)\) are convolutional layers specific to bounding box and class prediction tasks, respectively.</p>
  </li>
</ul>

<p>The output of the head is a set of bounding boxes and associated class probabilities:</p>

\[\text{Output} = \{(B_1, C_1), (B_2, C_2), \dots, (B_n, C_n)\}\]

<p>Where each pair \( (B_k, C_k) \) contains the bounding boxes and class probabilities for scale \( k \).</p>

<p>In YOLOv8, term “scale” refers to the different levels of detail the model examines to detect both small and large objects. For example, a feature map with a resolution of \(128 \times 128\) pixels is at a different scale compared to one with \(32 \times 32\) pixels. By combining these details, it can accurately find and identify objects of various sizes in an image.</p>

<p>To summarize:</p>

<ul>
  <li>
    <p>Backbone: \( F_l = \sigma(W_l \ast F_{l-1} + b_l), \quad l = 1, 2, \dots, L \)</p>
  </li>
  <li>
    <p>Neck: \( P_k = \text{Upsample}(F_{L}^k) + \text{Conv}(F_{L-1}^k), \quad k = 1, 2, \dots, n \)</p>
  </li>
  <li>
    <p>Head: \(B_k = \text{Conv}_\text{bbox}(P_k), \quad C_k = \text{Conv}_\text{class}(P_k)\)</p>
  </li>
</ul>

<p>These algebraic representations provide a formal view of the operations involved in the YOLOv8 architecture, capturing the transformation of the input image through feature extraction, aggregation, and finally, object detection.</p>

<blockquote>
  <p>YOLOv8: Fast and Versatile</p>
</blockquote>

<p>YOLOv8 is compatible with popular deep learning frameworks like PyTorch and TensorFlow. It also supports  deployment on edge devices, such as Nvidia Jetson, making it versatile for various use cases:</p>

<ul>
  <li>Surveillance systems for real-time monitoring and threat detection.</li>
  <li>Industrial inspection for identifying defects on assembly lines.</li>
  <li>Autonomous vehicles for detecting pedestrians, vehicles, and road signs.</li>
</ul>

    </div>
  </article>

  

  
    <h2>References</h2>
    <div class="publications">
      <h2 class="bibliography">2020</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="bochkovskiy2020yolov4" class="col-sm-8">
    <!-- Title -->
    <div class="title">Yolov4: Optimal speed and accuracy of object detection</div>
    <!-- Author -->
    <div class="author">
      

      
      Alexey
            Bochkovskiy, Chien-Yao
            Wang, and Hong-Yuan Mark
            Liao
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv preprint arXiv:2004.10934</em>,  Sep 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2004.10934" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of  65 FPS on Tesla V100.</p>
      </div>
    

    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2016</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="Redmon2016CVPR" class="col-sm-8">
    <!-- Title -->
    <div class="title">You Only Look Once: Unified, Real-Time Object Detection</div>
    <!-- Author -->
    <div class="author">
      

      
      Joseph
            Redmon, Santosh
            Divvala, Ross
            Girshick, and
        <span class="more-authors" title="click to view 1 more author" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '1 more author' ? 'Ali Farhadi' : '1 more author';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">1 more author</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>,  Jun 2016
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
        
          <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.</p>
      </div>
    

    

    
  </div>
</div>
</li></ol>
    </div>
  

  
    
      

  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/ai-on-jetson/">AI on Jetson</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/pbdl-jupyter/">Deeply Learning Physics</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/jetson-evolution/">The Jetson Evolution</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/rooflinemodel/">Bounded Performance: Uncovering the Roofline Model</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/nonlinear/">Neural Nonlinearities</a>
  </li>



    
  

  
  
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      Algotechniq
      
      . 
      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    
  <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script>
    let searchTheme = determineComputedTheme();
    const ninjaKeys = document.querySelector('ninja-keys');

    if (searchTheme === 'dark') {
      ninjaKeys.classList.add('dark');
    } else {
      ninjaKeys.classList.remove('dark');
    }

    const openSearchModal = () => {
      // collapse navbarNav if expanded on mobile
      const $navbarNav = $('#navbarNav');
      if ($navbarNav.hasClass('show')) {
        $navbarNav.collapse('hide');
      }
      ninjaKeys.open();
    };
  </script>
  <script>
    // get the ninja-keys element
    const ninja = document.querySelector('ninja-keys');

    // add the home and posts menu items
    ninja.data = [{
        id: "nav-home",
        title: "home",
        section: "Navigation",
        handler: () => {
          window.location.href = "/";
        },
      },{id: "nav-projects",
              title: "projects",
              description: "A growing collection of Algotechniq projects",
              section: "Navigation",
              handler: () => {
                window.location.href = "/projects/";
              },
            },{id: "nav-blog",
              title: "blog",
              description: "",
              section: "Navigation",
              handler: () => {
                window.location.href = "/blog/";
              },
            },{id: "nav-about",
              title: "about",
              description: "Summary description of Algotechniq LLC.",
              section: "Navigation",
              handler: () => {
                window.location.href = "/cv/";
              },
            },{id: "post-ai-on-jetson",
          
            title: "AI on Jetson",
          
          description: "In four bipedal steps",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2024/ai-on-jetson/";
            
          },
        },{id: "post-yolo",
          
            title: "YOLO",
          
          description: "Illustrating a fast and accurate object detection algorithm",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2024/yolov8/";
            
          },
        },{id: "post-deeply-learning-physics",
          
            title: "Deeply Learning Physics",
          
          description: "A few notes in a Jupyter notebook",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2024/pbdl-jupyter/";
            
          },
        },{id: "post-the-jetson-evolution",
          
            title: "The Jetson Evolution",
          
          description: "As Nvidia evolves Jetson, embedded systems thrive",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2024/jetson-evolution/";
            
          },
        },{id: "post-bounded-performance-uncovering-the-roofline-model",
          
            title: "Bounded Performance: Uncovering the Roofline Model",
          
          description: "Is your AI algorithm computation-bound or memory-bound?",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/assets/pdf/roof.pdf";
            
          },
        },{id: "post-neural-nonlinearities",
          
            title: "Neural Nonlinearities",
          
          description: "What makes neural nets so powerful?",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/assets/pdf/nonlinear.pdf";
            
          },
        },{id: "post-ai-39-s-carbon-footprint",
          
            title: "AI's Carbon Footprint",
          
          description: "The quintillions are alarming",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2023/ai-carbon-footprint/";
            
          },
        },{id: "post-the-bottom-up-magic-of-deep-learning",
          
            title: "The Bottom-Up Magic of Deep Learning",
          
          description: "From alchemy to architecture",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2023/dnn-bottomup/";
            
          },
        },{id: "post-crafting-neural-networks",
          
            title: "Crafting Neural Networks",
          
          description: "Building layers bottom-up",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2023/nn-zoo/";
            
          },
        },{id: "post-claws-of-concern",
          
            title: "Claws of Concern",
          
          description: "Bears attacking Japan",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2023/bears-attack/";
            
          },
        },{id: "news-a-simple-inline-announcement",
              title: 'A simple inline announcement.',
              description: "",
              section: "News",},{id: "news-a-long-announcement-with-details",
              title: 'A long announcement with details',
              description: "",
              section: "News",handler: () => {
                  window.location.href = "/news/announcement_2/";
                },},{id: "news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",
              title: 'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',
              description: "",
              section: "News",},{id: "projects-highsafe",
              title: 'HighSafe',
              description: "Instant accident alerts for safer roads",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/1_project/";
                },},{id: "projects-deephole",
              title: 'DeepHole',
              description: "Real-time road quality detection on Edge",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/2_project/";
                },},{id: "projects-concretevision",
              title: 'ConcreteVision',
              description: "Real-time concrete pouring monitor",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/3_project/";
                },},{id: "projects-smartscan",
              title: 'SmartScan',
              description: "Detecting surface defects with deep learning",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/4_project/";
                },},{id: "projects-autoclaim-ai",
              title: 'AutoClaim AI',
              description: "Precision damage detection for smarter insurance",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/5_project/";
                },},{id: "projects-wildsight",
              title: 'WildSight',
              description: "Precision wildlife detection with deep learning",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/6_project/";
                },},{
            id: 'socials-email',
            title: 'Send email',
            section: 'Socials',
            handler: () => {
              window.open("mailto:%6B%61%6D%62%69%7A@%67%6D%61%69%6C.%63%6F%6D", "_blank");
            },
          },{
            id: 'socials-google-scholar',
            title: 'Google Scholar',
            section: 'Socials',
            handler: () => {
              window.open("https://scholar.google.com/citations?user=dV8LzD8AAAAJ", "_blank");
            },
          },];
  </script>
  <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>


  </body>
</html>
